{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrmFm3MdcIY8",
        "outputId": "f3f9f556-6cfc-4b3e-835e-199028fcecb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'seminar-dlmb-rnn'...\n",
            "remote: Enumerating objects: 186, done.\u001b[K\n",
            "remote: Counting objects: 100% (186/186), done.\u001b[K\n",
            "remote: Compressing objects: 100% (114/114), done.\u001b[K\n",
            "remote: Total 186 (delta 72), reused 173 (delta 62), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (186/186), 10.51 MiB | 15.13 MiB/s, done.\n",
            "Resolving deltas: 100% (72/72), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/dev-meesjakob/seminar-dlmb-rnn.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/seminar-dlmb-rnn/models # create the directory to save models in"
      ],
      "metadata": {
        "id": "xE2_nhe_FvQp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the protein sequence data\n",
        "with open(\"/content/seminar-dlmb-rnn/dataset/data.txt\", \"r\") as f:\n",
        "    sequences = f.readlines()\n",
        "\n",
        "# Read the labels\n",
        "with open(\"/content/seminar-dlmb-rnn/dataset/labels.txt\", \"r\") as f:\n",
        "    labels = f.readlines()\n",
        "\n",
        "# Remove newline characters\n",
        "sequences = [seq.strip() for seq in sequences]\n",
        "labels = [label.strip() for label in labels]\n",
        "\n",
        "# Convert labels to integers\n",
        "labels = [int(label) for label in labels]\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'sequence': sequences,\n",
        "    'label': labels\n",
        "})\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jqXws07vcOgl",
        "outputId": "c6008bd6-860e-4713-e3e2-b08f79027c1e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         sequence  label\n",
              "0                      ANDENYALAA      0\n",
              "1         MPKTKPKVKNHKRNKTEPSPKQP      0\n",
              "2                  LKHFEDWSTAMLTA      0\n",
              "3  MLISAYPKVSLGMVKLVLMVDLSAPKRLGG      0\n",
              "4            RNAHNFPLDLAAIEAPSTNG      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-365bf4ca-3e7e-460f-a490-bf816eaef084\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ANDENYALAA</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MPKTKPKVKNHKRNKTEPSPKQP</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LKHFEDWSTAMLTA</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MLISAYPKVSLGMVKLVLMVDLSAPKRLGG</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RNAHNFPLDLAAIEAPSTNG</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-365bf4ca-3e7e-460f-a490-bf816eaef084')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-365bf4ca-3e7e-460f-a490-bf816eaef084 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-365bf4ca-3e7e-460f-a490-bf816eaef084');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3c5065e9-ee19-43f2-88d2-0615ec55b2ae\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c5065e9-ee19-43f2-88d2-0615ec55b2ae')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3c5065e9-ee19-43f2-88d2-0615ec55b2ae button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 13766,\n  \"fields\": [\n    {\n      \"column\": \"sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12976,\n        \"samples\": [\n          \"SNKKDYRKEIVDKHNALSRSVKPTASNM\",\n          \"TRSGGACNSHNQCCDDFCSTATSTCI\",\n          \"DFFGLLTKVKCDRRGAGVEAGF\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_label_split(df, target_col):\n",
        "  y = df[[target_col]]\n",
        "  X = df.drop(columns=[target_col])\n",
        "  return X, y\n",
        "\n",
        "def train_val_test_split(df, target_col, test_ratio):\n",
        "    validation_ratio = test_ratio / (1 - test_ratio)\n",
        "    X, y = feature_label_split(df, target_col)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_ratio, shuffle=False)\n",
        "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_ratio, shuffle=False)\n",
        "\n",
        "    return X_train, X_validation, X_test, y_train, y_validation, y_test\n",
        "\n",
        "#X_train, X_validation, X_test, y_train, y_validation, y_test = train_val_test_split(df, 'label', 0.25)"
      ],
      "metadata": {
        "id": "Rv5Eeyj2wQIk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train = torch.utils.data.TensorDataset(torch.Tensor(X_train),torch.Tensor(y_train))\n",
        "#validation = torch.utils.data.TensorDataset(torch.Tensor(X_validation),torch.Tensor(y_validation))\n",
        "#test = torch.utils.data.TensorDataset(torch.Tensor(X_test),torch.Tensor(y_test))"
      ],
      "metadata": {
        "id": "IH0eydTRwii6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test = train_test_split(df, train_size=0.8, shuffle=True) # shuffle=True is important here since the raw data is ordered negative to positive\n",
        "df_train, df_val = train_test_split(df_train, test_size=0.25, shuffle=True) # split the training set into training and validation\n",
        "\n",
        "df_train.reset_index(drop=True, inplace=True)\n",
        "df_test.reset_index(drop=True, inplace=True)\n",
        "df_val.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "hRTi5TrncTKD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_acids = ['A','R','N','D','C','Q','E','G','H','I','L','K','M','F','P','O','S','U','T','W','Y','V','B','Z','X','J','0'] # each letter corresponds to an amino acid, x is an unknown acid and 0 will be used for padding\n",
        "n_acids = len(all_acids)"
      ],
      "metadata": {
        "id": "ahTNLOl4cVzw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "targets = le.fit_transform(all_acids)\n",
        "\n",
        "all_acids_tensor = torch.Tensor(targets)\n",
        "one_hot_acids = F.one_hot(all_acids_tensor.long()) # this lets us encode every acid as a one hot vector of length n_acids\n",
        "\n",
        "# returns the one hot vector corresponding to the input\n",
        "def acid_to_tensor(acid):\n",
        "  tensor = torch.zeros(1, n_acids)\n",
        "  tensor[0] = one_hot_acids[all_acids.index(acid)]\n",
        "  return tensor\n",
        "\n",
        "# returns a tensor of one hot vectors for the input protein\n",
        "def protein_to_tensor(protein):\n",
        "  tensor = torch.zeros(len(protein), 1, n_acids)\n",
        "  for li, acid in enumerate(protein):\n",
        "    tensor[li] = acid_to_tensor(acid)\n",
        "  return tensor"
      ],
      "metadata": {
        "id": "dAyOVkPlcY8y"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ToxinDatasetReader(Dataset):\n",
        "  def __init__(self, df: pd.DataFrame):\n",
        "    self.df = df\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, idx: int):\n",
        "    row = self.df.loc[idx]\n",
        "    input_protein = list(row.sequence)\n",
        "    len_protein = len(input_protein)\n",
        "    labels = row.label\n",
        "    return input_protein, len_protein, labels\n",
        "\n",
        "train_dat = ToxinDatasetReader(df_train)\n",
        "test_dat = ToxinDatasetReader(df_test)\n",
        "val_dat = ToxinDatasetReader(df_val)"
      ],
      "metadata": {
        "id": "4ikIfy66cbIi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch_rnn(data):\n",
        "  _, lengths, labels  = zip(*data)\n",
        "  max_len = max(lengths)\n",
        "\n",
        "  features = torch.zeros(len(data), max_len, n_acids)\n",
        "  labels = torch.tensor(labels)\n",
        "  lengths = torch.tensor(lengths)\n",
        "\n",
        "  for i in range(len(data)):\n",
        "    padding = [\"0\"] * (max_len - len(data[i][0]))\n",
        "    data[i][0].extend(padding)\n",
        "\n",
        "  for i in range(len(data)):\n",
        "    for j in range(len(data[i][0])):\n",
        "      features[i][j] = acid_to_tensor(data[i][0][j])\n",
        "\n",
        "  return features.float(), lengths.long(), labels.long()"
      ],
      "metadata": {
        "id": "xu8sUhWpccue"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32 # this should be tweaked and tested\n",
        "train_dat_loader = DataLoader(train_dat, batch_size=batch_size, collate_fn=collate_batch_rnn, drop_last=True)\n",
        "test_dat_loader = DataLoader(test_dat, batch_size=batch_size, collate_fn=collate_batch_rnn, drop_last=True)\n",
        "val_dat_loader = DataLoader(val_dat, batch_size=batch_size, collate_fn=collate_batch_rnn, drop_last=True)\n",
        "\n",
        "# test the dataloader\n",
        "proteins_tensor, lengths, labels = next(iter(train_dat_loader)) # this gives a tensor containing 32 protein tensors which are all padded to be the same length as the longest in the batch\n",
        "\n",
        "print(labels.size(0)) # tensor of dimension <batch_size x max_length x n_acids>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihDoNVlecehf",
        "outputId": "1d89ca07-6b76-4898-e4af-6deb0e627ba4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using '{device}' device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BlWuOAVkSQ-",
        "outputId": "b01a6de9-915b-4381-90b4-651bd6be890d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 'cpu' device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    #self.input2hidden = nn.Linear(input_size, hidden_size, bias=False)\n",
        "    #self.hidden2hidden = nn.Linear(hidden_size, hidden_size)\n",
        "    #self.hidden2output = nn.Linear(hidden_size, output_size)\n",
        "    self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x, lengths, hidden_state=None):\n",
        "    if hidden_state is None:\n",
        "      hidden_state = self.init_hidden(x.size(0))\n",
        "\n",
        "    hidden_state.to(device)\n",
        "\n",
        "    lengths, perm_idx = lengths.sort(0, descending=True)\n",
        "    x = x[perm_idx]\n",
        "\n",
        "    packed_input = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True).to(device)\n",
        "\n",
        "    packed_output, hidden_state = self.rnn(packed_input, hidden_state)\n",
        "\n",
        "    output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "    _, unperm_idx = perm_idx.sort(0, descending=False)\n",
        "    output = output[unperm_idx]\n",
        "\n",
        "    output = self.fc(output[:, -1, :])\n",
        "    output = self.sigmoid(output)\n",
        "\n",
        "    return output, hidden_state\n",
        "\n",
        "  def _process_packed_input(self, packed_input, hidden_state):\n",
        "    outputs = []\n",
        "    batch_sizes = packed_input.batch_sizes\n",
        "\n",
        "    input_offset = 0\n",
        "    last_batch_size = batch_sizes[0]\n",
        "    hiddens = hidden_state[:last_batch_size]\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "      current_input = packed_input.data[input_offset:input_offset + batch_size]\n",
        "      current_hidden = hidden_state[:batch_size]\n",
        "\n",
        "      if batch_size != last_batch_size:\n",
        "        hiddens = hidden_state[:batch_size]\n",
        "        last_batch_size = batch_size\n",
        "\n",
        "      x = self.input2hidden(current_input)\n",
        "      hiddens = self.hidden2hidden(hiddens)\n",
        "      hiddens = torch.tanh(x + hiddens)\n",
        "\n",
        "      outputs.append(hiddens)\n",
        "      input_offset += batch_size\n",
        "\n",
        "    packed_output = torch.cat(outputs, dim=0)\n",
        "    packed_output = nn.utils.rnn.PackedSequence(packed_output, batch_sizes)\n",
        "\n",
        "    return packed_output, hidden_state\n",
        "\n",
        "  def init_hidden(self, batch_size=1):\n",
        "    return torch.zeros(1, batch_size, self.hidden_size, requires_grad=False).to(device)"
      ],
      "metadata": {
        "id": "7HH35qJUrVnN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Optimization:\n",
        "  def __init__(self, model, loss_fn, optimizer):\n",
        "    self.model = model\n",
        "    self.loss_fn = loss_fn\n",
        "    self.optimizer = optimizer\n",
        "    self.train_losses = []\n",
        "    self.val_losses = []\n",
        "\n",
        "  def train_step(self, batch, lengths, labels):\n",
        "    batch = batch.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    self.model.train()\n",
        "    outputs, _ = self.model(batch, lengths)\n",
        "\n",
        "    labels = labels.view(-1, 1).float()\n",
        "\n",
        "    loss = self.loss_fn(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    self.optimizer.step()\n",
        "    self.optimizer.zero_grad()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "  def train(self, train_loader, val_loader, batch_size=32, n_epochs=50, n_features=1):\n",
        "    model_path = f'/content/seminar-dlmb-rnn/models/{self.model}_{datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\").replace(\" \", \"_\")}'\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        batch_losses = []\n",
        "\n",
        "        # training set\n",
        "        for batch, lengths, labels in train_loader:\n",
        "          loss = self.train_step(batch, lengths, labels)\n",
        "          batch_losses.append(loss)\n",
        "\n",
        "        training_loss = np.mean(batch_losses)\n",
        "        self.train_losses.append(training_loss)\n",
        "\n",
        "        # validation set\n",
        "        with torch.no_grad():\n",
        "          batch_val_losses = []\n",
        "          for batch, lengths, labels in val_loader:\n",
        "            batch = batch.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            labels = labels.view(-1, 1).float()\n",
        "\n",
        "            self.model.eval()\n",
        "            outputs, _ = self.model(batch, lengths)\n",
        "\n",
        "            val_loss = self.loss_fn(outputs, labels).item()\n",
        "            batch_val_losses.append(val_loss)\n",
        "\n",
        "          validation_loss = np.mean(batch_val_losses)\n",
        "          self.val_losses.append(validation_loss)\n",
        "\n",
        "        if (epoch <= 10) | (epoch % 50 == 0):\n",
        "          print(f\"[{epoch}/{n_epochs}] Training loss: {training_loss:.4f}\\t Validation loss: {validation_loss:.4f}\")\n",
        "\n",
        "    torch.save(self.model.state_dict(), model_path)\n",
        "\n",
        "  def evaluate(self, test_loader, batch_size=1, n_features=1):\n",
        "\n",
        "    # test set\n",
        "    with torch.no_grad():\n",
        "      predictions = []\n",
        "      values = []\n",
        "\n",
        "      for batch, lengths, labels in test_loader:\n",
        "        batch = batch.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        self.model.eval()\n",
        "        outputs = self.model(batch, lengths)\n",
        "        predictions.append(outputs.detach().numpy())\n",
        "        values.append(labels.detach().numpy())\n",
        "\n",
        "      return predictions, values\n",
        "\n",
        "  def plot_losses(self):\n",
        "    plt.plot(self.train_losses, label=\"Training loss\")\n",
        "    plt.plot(self.val_losses, label=\"Validation loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Losses\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "sXZv78BNdL53"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import glob\n",
        "import os\n",
        "\n",
        "input_size = n_acids\n",
        "output_size = 1\n",
        "hidden_size = 64\n",
        "n_epochs = 10\n",
        "learning_rate = 0.001\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "model_params = {'input_size': input_size,\n",
        "                'hidden_size' : hidden_size,\n",
        "                'output_size' : output_size\n",
        "                }\n",
        "\n",
        "model = RNN(**model_params)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Define the directory containing the models\n",
        "model_directory = '/content/seminar-dlmb-rnn/models/'\n",
        "\n",
        "# Use glob to find files starting with \"model_\"\n",
        "model_files = glob.glob(os.path.join(model_directory, 'model_*'))\n",
        "\n",
        "# Check if any model files were found\n",
        "if model_files:\n",
        "    # Find the latest file based on modification time\n",
        "    latest_model_file = max(model_files, key=os.path.getmtime)\n",
        "    model_path = latest_model_file\n",
        "    print(f'Latest model file found: {model_path}')\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "else:\n",
        "    print('No model files found starting with \"model_\"')\n",
        "\n",
        "opt = Optimization(model=model, loss_fn=loss_fn, optimizer=optimizer)\n",
        "opt.train(train_dat_loader, val_dat_loader, n_epochs=n_epochs, n_features=input_size)\n",
        "opt.plot_losses()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "I2_1fSnaOnRN",
        "outputId": "24770f44-74b9-4a49-b259-27c94bc01b70"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No model files found starting with \"model_\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-77bf8d7cd126>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dat_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dat_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-b47464271511>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, batch_size, n_epochs, n_features)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m           \u001b[0mbatch_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-b47464271511>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, batch, lengths, labels)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}